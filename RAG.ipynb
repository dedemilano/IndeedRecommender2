{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import getpass\n",
    "import os\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain_core.documents import Document \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "from langgraph.graph import START, StateGraph, MessagesState\n",
    "from typing_extensions import List, TypedDict\n",
    "import subprocess\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from typing import Literal, Dict, Any\n",
    "from typing_extensions import Annotated\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "import concurrent.futures\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "import json\n",
    "import gc\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTRALAI_API_KEY : vVA7pch9ZTp18pgndGCtyzwyem1myi28\n",
      "LANGCHAIN_API_KEY : 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\n",
      "ANTHROPIC_API_KEY : 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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# Utiliser les variables\n",
    "MISTRALAI_API_KEY = os.getenv(\"MISTRALAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "print(\"MISTRALAI_API_KEY :\", MISTRALAI_API_KEY)\n",
    "print(\"LANGCHAIN_API_KEY :\", LANGCHAIN_API_KEY)\n",
    "print(\"ANTHROPIC_API_KEY :\", ANTHROPIC_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def reset_chroma_db():\n",
    "    \"\"\"Supprime la base de données Chroma existante\"\"\"\n",
    "    if os.path.exists(\"./chroma_db\"):\n",
    "        shutil.rmtree(\"./chroma_db\")\n",
    "        print(\"Base de données Chroma réinitialisée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Chargement optimisé des données\n",
    "def load_json_data(file_path='./indeedJobData.json', batch_size=1000):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        print(f\"Data is a list of {len(data)} items\")\n",
    "        print(\"\\nFirst item keys:\", data[0].keys())\n",
    "        \n",
    "        loader = JSONLoader(\n",
    "            file_path=file_path,\n",
    "            jq_schema='.[]',\n",
    "            text_content=False\n",
    "        )\n",
    "        return loader, len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Préparation des documents par lots\n",
    "def prepare_documents_batch(loader, batch_size=1000):\n",
    "    docs_lazy = loader.lazy_load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,  # Augmenté pour réduire le nombre de splits\n",
    "        chunk_overlap=100  # Réduit pour économiser la mémoire\n",
    "    )\n",
    "    \n",
    "    batch = []\n",
    "    splits = []\n",
    "    \n",
    "    for doc in docs_lazy:\n",
    "        batch.append(doc)\n",
    "        if len(batch) >= batch_size:\n",
    "            batch_splits = text_splitter.split_documents(batch)\n",
    "            splits.extend(batch_splits)\n",
    "            batch = []\n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "    \n",
    "    # Traiter le dernier lot si nécessaire\n",
    "    if batch:\n",
    "        batch_splits = text_splitter.split_documents(batch)\n",
    "        splits.extend(batch_splits)\n",
    "    \n",
    "    print(f\"Total splits created: {len(splits)}\")\n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configuration des embeddings\n",
    "def setup_embeddings():\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Configuration et ajout au vectorstore par lots\n",
    "def setup_and_add_to_vectorstore(splits, embeddings, batch_size=100):\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"indeedChromaCollection\",\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=\"./chroma_db\"\n",
    "    )\n",
    "    total_batches = len(splits) // batch_size + (1 if len(splits) % batch_size else 0)\n",
    "    \n",
    "    for i in tqdm(range(0, len(splits), batch_size), desc=\"Adding documents\", total=total_batches):\n",
    "        batch = splits[i:i + batch_size]\n",
    "        try:\n",
    "            vector_store.add_documents(batch)\n",
    "            vector_store.persist()  # Sauvegarder après chaque lot\n",
    "            time.sleep(0.1)  # Petit délai pour éviter la surcharge\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding batch {i//batch_size + 1}: {e}\")\n",
    "            continue\n",
    "        gc.collect()  # Force garbage collection après chaque lot\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Configuration du LLM\n",
    "def setup_llm():\n",
    "    return ChatAnthropic(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        anthropic_api_key=ANTHROPIC_API_KEY,\n",
    "        temperature=0.7\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Création de la chaîne RAG\n",
    "def create_rag_chain(vector_store, llm):\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 3}\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Tu es un assistant spécialisé dans l'analyse d'offres d'emploi. \n",
    "        Utilise le contexte fourni pour répondre aux questions de manière précise.\n",
    "        \n",
    "        Contexte: {context}\n",
    "        Question: {question}\"\"\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    \n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever,\n",
    "            \"question\": itemgetter(\"question\")\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return chain, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Fonction principale d'initialisation\n",
    "def initialize_rag_system(batch_size=1000):\n",
    "    print(\"Loading data...\")\n",
    "    loader, total_docs = load_json_data(batch_size=batch_size)\n",
    "    \n",
    "    print(\"\\nPreparing documents in batches...\")\n",
    "    splits = prepare_documents_batch(loader, batch_size=batch_size)\n",
    "    \n",
    "    print(\"\\nSetting up embeddings...\")\n",
    "    embeddings = setup_embeddings()\n",
    "    \n",
    "    print(\"\\nSetting up vector store and adding documents...\")\n",
    "    vector_store = setup_and_add_to_vectorstore(splits, embeddings)\n",
    "    \n",
    "    print(\"\\nSetting up LLM...\")\n",
    "    llm = setup_llm()\n",
    "    \n",
    "    print(\"\\nCreating RAG chain...\")\n",
    "    chain, retriever = create_rag_chain(vector_store, llm)\n",
    "    \n",
    "    return chain, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Fonction de requête\n",
    "def query_rag_system(chain, retriever, question: str):\n",
    "    try:\n",
    "        response = chain.invoke({\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        context_docs = retriever.get_relevant_documents(question)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response,\n",
    "            \"sources\": [doc.metadata for doc in context_docs]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying RAG system: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data is a list of 37147 items\n",
      "\n",
      "First item keys: dict_keys(['positionTitle', 'companyName', 'location', 'salaryInfoAndJobType', 'description', 'datePosted', 'salary', 'jobType', 'shiftAndSchedule', 'jobBenefits', 'updatedDate', 'country', 'profession', 'language'])\n",
      "\n",
      "Preparing documents in batches...\n",
      "Total splits created: 136066\n",
      "\n",
      "Setting up embeddings...\n",
      "WARNING:tensorflow:From c:\\Users\\gueid\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "Setting up vector store and adding documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents:   0%|          | 0/1361 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialiser avec un batch_size plus petit\n",
    "    chain, retriever = initialize_rag_system(batch_size=500)\n",
    "    \n",
    "    # Questions de test\n",
    "    questions = [\n",
    "        \"Quels sont les emplois les plus demandés ?\",\n",
    "        \"Quelles sont les compétences les plus recherchées ?\",\n",
    "        \"Quel est le salaire moyen proposé ?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting the system with sample questions...\")\n",
    "    for question in questions:\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        try:\n",
    "            result = query_rag_system(chain, retriever, question)\n",
    "            print(f\"Réponse: {result['answer']}\")\n",
    "            print(\"\\nSources utilisées:\")\n",
    "            for source in result['sources']:\n",
    "                print(f\"- {source}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Fatal error during initialization: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
